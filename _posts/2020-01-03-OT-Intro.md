---
layout: post
title: Optimal Transport Preliminaries
date:   2020-01-03 17:00:00 -0500
---
<!--more-->

## Overview:
In this post, I'll be going over some of the basics of optimal transport theory. This is meant to help me to understand the problem that the study of optimal transport seeks to solve, as I will be diving deeper into this topic over the next 12-18 months for my senior thesis in mathematics. I've only just started to study this topic, so I'm not sure where I'd like to direct my energy quite yet. In future posts, I'll be experimenting programatically with some of the underlying mechanics in the implementation of OT-inspired techniques in modern machine learning pipelines. For now, though, we'll start with a brief fly-over of the optimal transport problem(s), which I've condensed from the first part of [this lecture](https://vimeo.com/248504509) given by Justin Solomon at the 2017 Conference on Neural Information Processing Systems.


___
### Motivation:
The fundamental problem lying at the core of the study of Optimal Transport is a generalization of the shortest-path problem, which arises frequently in both academia and in daily life. This problem is concerned with finding the best way best way to move an object from point $A$ to point $B$, where we can modify our definition of "best" to suit our needs. Airlines want to find the most cost-efficient route for an airplane, online retailers want to find the quickest way to get a package from the warehouse to the consumer, data engineers want to find the most secure, expeditious way to move a data item to and from the cloud, and you want to find the shortest path to walk across a room. In OT, though, we shift our focus to searching for the best way to move a _group of objects_ from point $A$ to point $B$. This problem is much more complex and requires the consideration many different transport plans, as our space of possibilites expands with the permuatations of objects and routes. But that's the version of the problem that we see most frequently in our daily lives. We want to move many things to many different destinations, and it's common for there to be an infinite number of plans to consider. OT seeks to  mathematically formalize methods for computing the plan that's best-suited for any given transportation problem.

___
### The Monge Formulation:
The first formulation of the optimal transport problem was posed by French mathematician Gaspard Monge in the late-18th century. From a high level, Monge was concerned with finding a way to minimize the overall "cost" of moving one pile of sand into a nearby hole in the ground with the same volume as the pile. This problem may sound inconsequential but, mathematically, it has been shown to vast implications. We can formalize the Monge formulation of the problem to understand why this is the case. Mathematicians are concerned with finding the minimal global cost of transforming one probability distribution into another, where the two distributions represent the piles of sand from Monge's example. The basic approach is to consider all of the possible ways to morph one distribution to the other, calculate the cost of doing so in each case, and choose the transport plan that minimizes this cost. Here is our set-up: suppose there is an arbitrary probability space $\Omega$, a cost function $c\text{ }\colon\Omega \times \Omega \to \mathbb{R}$, and two probability measures $\mu, \nu \in \mathcal{P}(\Omega)$. Then the Monge problem is to find the map $T\colon\Omega \to \Omega$ that realizes the following infimum:

$$\inf_{T_{\sharp}\mu = \nu}{\int_{\Omega}{c(T(x), x))\mu(dx)}},$$

where $c(T(x), x)$ is the cost of the map $x \mapsto T(x)$ and $\mu(dx)$ is the amount of mass pushed forward from $x$ to $T(x)$. The constraint $T_{\sharp}\mu = \nu$ is short-hand notation for requiring that "$T$ pushes $\mu$ forward to $\nu$". Formally this means that, for every space $B \in \Omega$, $\mu(T^{-1}(B)) = \nu(B)$. Put simply, we require that $T$ moves every grain of sand from $\mu$ to $\nu$. We must note, though, that an optimal solution $T^{\star}$ to this problem is not unique in general.  We can visualize a simplified instance of this problem in the diagram below (letting $T(x) = y$), taken from [this paper](https://hal.archives-ouvertes.fr/hal-01717943/document):

<div class="img-container">
<img src="/post_assets/2020-01-03/mass_transport.png" style="width:400px">
</div>

It can be shown that, if $\Omega = \mathbb{R}^d$, $c$ is defined to be the $L^2$ norm, and $\mu, \nu$ are absolutely continuous, then $T$ exists and is equal to $\nabla{u}$ for some convex function $u$. However, there is no guarantee of the existence of a map $T$ that realizes the infimum in general, as the constraint of absolute continuity is crucial. 

___
### The Kantorovich Formulation
In the late-20th century, Soviet mathematician Leonid Kantorovich developed a reformulation of the Monge problem that provides us with more relaxed constraints for the guarantee of a solution which can be obtained with a known gradient descent algorithm. For this formulation, we now consider probablistic couplings $P \in \mathcal{P}(\Omega \times \Omega)$. Define the set $\Pi(\mu, \nu)$ of all such couplings as follows:

$$ \Pi(\mu, \nu) := \{ P \in \mathcal{P}(\Omega \times \Omega) \mid \forall A, B \subset \Omega, P(A \times \Omega) = \mu(A) \text{ and } P(\Omega \times B) = \nu(B) \}$$

This looks a bit complex, but we're just reformulating the same requirement that Monge set out for us: we need each coupling $P$ in the probability space on the product of $\Omega$ and itself to cover the entirety of $\mu$ and map it to the entirety of $\nu$. In this case, though, we are allowed to split mass up to transport it. We can observe that $\Pi(\mu, \nu)$ is always non-empty, as the trivial transport plan $\mu \otimes \nu$ We can think of a measure coupling $P$ like a function on a square; for example, $P(i, j)$ gives us the amount of sand moved from location $i$ to location $j$. An illustration of a possible transport plan (pairing) is shown below, taken from [this lecture](https://www.dropbox.com/s/55tb2cf3zipl6xu/aprimeronOT.pdf?dl=0):

<div class="img-container">
<img src="/post_assets/2020-01-03/kantorovich.png" style="width:400px">
</div>

Now that this formulation is much less intimidating, we can state the relaxation of this problem. Recall the set-up from before: there is an arbitrary probability space $\Omega$, a cost function $c\text{ }\colon\Omega \times \Omega \to \mathbb{R}$, and two probability measures $\mu, \nu \in \mathcal{P}(\Omega)$. Then, given the Kantorovich problem is to find the probability measure $P \in \mathcal{P}(\Omega \times \Omega)$ that realizes the following infimum:

$$ \inf_{P \in \Pi(\mu, \nu)}{\int_{\Omega \times \Omega}{c(x,y)P(dx,dy)}}$$

If we consider $X$ and $Y$ to be two random variables distributed by $P$ over $\Omega$, then we can view this problem through a probablistic lens in the following way: we wish to find a $P$ that realizes the following infimum:

$$\inf_{P \in \Pi(\mu, \nu)}{\mathbb{E}_P[c(X,Y)]}$$

Thus, we're really just looking to minimize the expectation of the cost of pushing $X$ forward to $Y$. Again, we note that an optimal solution $P^{\star}$ to this problem is not unique in general. However, we can observe that the set of transport plans an interesting result of the Kantorovich formulation of the problem is that, for a "well-behaved" cost function $c$, if one finds an optimal solution $T^{\star}$ to the Monge problem, then an optimal solution to the Kantorovich problem is $P^{\star} = (Id, T^{\star})_{\sharp}\mu$. Note that $P^{\star}$ is just a thinly-concentrated coupling in $\Pi(\mu, \nu)$. In cases where the cost and densities are extremely "well-behaved" (more on these constraints to come in following posts), then a Kantorovich solution solves the Monge problem, as well. This hints at an intimate relationship shared between the Monge and Kantorovich problems, and I'm hoping to dive deeper into this relationship in future posts.

___
### Kantorovich-Wasserstein Distances
An interesting case of the Kantorovich problem occurs when $c := D$ is a metric on some geometric space. That is, $D(x,y) \mapsto [0,\infty)$, $D$ is reflexive, and $D$ satisfies the triangle inequality. Then, we can define the __$p$-Wasserstein Distance__ between $\mu$ and $\nu$ in $\mathcal{P}(\Omega)$ as follows:

$$W_p(\mu, \nu) := \Big(\inf_{P \in \Pi(\mu, \nu)}{\int_{\Omega \times \Omega}{D(x,y)^pP(dx,dy)}}\Big)^{1/p}$$

Then, given an optimal solution $P^\star$ to the Kantorovich problem with a valid metric $D$, the $p$-th root of the infimum achieved by that solution (raising the metric to the $p$-th power) satisfies the triangle inequality, provided that $p \geq 1$. We note that the operator $W$ is trivially reflexive, so the $p$-Wasserstein distance between probability measures is truly a distance (a metric). This is a _powerful_ result that lies at the core of optimal transport theory: the Wasserstein distance allows us to define an intuitive geometry on $\mathcal{P}(\Omega)$, and we can thus think of the set $\mathcal{P}(\Omega)$ as a high-dimensional manifold. Equipping $\mathcal{P}(\Omega)$ with the Wasserstein metric allows us to comfortably define a concept of distance between probability measures. Non-linear interpolation between measures with the Wasserstein distance is an interesting application of this result, as it provides us with a better picture of the "transition phases" in pushing a measure forward onto another. An illustration (taken from [this lecture](https://www.dropbox.com/s/55tb2cf3zipl6xu/aprimeronOT.pdf?dl=0)) of displacement interpolation is provided below:

<div class="img-container">
<img src="/post_assets/2020-01-03/interpolation.png" style="height:250px">
</div>

The linear interpolation ($L^1$/$L^2$ norm) between the blue measure on the left and the red measure on the right shows mass vanishing from the left and reappearing on the right, as we take the weighted averages of the linear shortest path between the two. The Wasserstein shortest distance, though, shows the blue distribution sliding onto the red distribution in a much more intuitive fashion, and we see from this why this might make for an interesting tool in machine learning pipelines, particularly in image recognition tasks. The Wasserstein distance is an intuitive, mathematically-sound tool for calculating the "difference" between probability measures, which arise frequently in the field of statistical learning.


___
### Afterthoughts:
I really like the feel of this problem. The examples that motivate the study of OT are concerned with optimizing the movement of objects in some space, and I've always been fascinated with such problems. Finding the optimal way to move trains, cars, planes, people, packages, money, and information is a problem of concern to every human on earth, and that encourages me to dig deeper into both the pure and applied aspects of this topic. In future posts, I'll be looking at the computational side of optimal transport that we've glossed over here, as well as further applications of the theory which has been described above. In recent years, the rich geometry provided by OT has been shown to be quite useful in image processing, graphics, and general machine learning, and I'll be looking at how OT fits into each of those fields. Time permitting, I'll attempt proofs of some of the results which were omitted above.